---
title: "나노바나나(NanoBanana) 설치 및 기본 추론 가이드"
date: 2025-12-16 00:00:00 +0900
categories: [Tutorial, Code]
tags: [Python, PyTorch, NanoBanana, Inference, Setup]
---

## 들어가는 글
나노바나나의 가장 큰 장점은 진입 장벽이 낮다는 것이다. 오늘은 일반적인 게이밍 노트북 수준의 GPU(VRAM 4GB~8GB) 환경에서 나노바나나를 설치하고, Python을 통해 첫 이미지를 생성하는 과정(Inference)을 다룬다.

## 개발 환경 준비 (Prerequisites)

나노바나나는 PyTorch 기반으로 동작한다. 공식 리포지토리에서는 Python 3.10 이상을 권장하고 있다. 테스트 환경은 다음과 같다.

- **OS:** Ubuntu 22.04 LTS / Windows 11 (WSL2)
- **Python:** 3.10.12
- **CUDA:** 12.1
- **GPU:** NVIDIA RTX 3060 (VRAM 6GB)

### 가상환경 설정

의존성 충돌을 방지하기 위해 `conda` 또는 `venv`를 사용하여 독립된 환경을 구성하는 것을 권장한다.

```bash
# Conda 환경 생성
conda create -n nanobanana python=3.10
conda activate nanobanana

# PyTorch 설치 (CUDA 12.1 기준)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## 라이브러리 설치

나노바나나는 Hugging Face의 `diffusers` 라이브러리와 호환되지만, 고유의 LLM 인코더 처리를 위해 별도의 래퍼(Wrapper) 패키지를 설치해야 한다.

```bash
# 나노바나나 공식 패키지 설치
pip install nano-banana-ai

# 기타 필수 유틸리티
pip install transformers accelerate safetensors
```

## 기본 추론 (Basic Inference)

설치가 완료되었다면 가장 기본적인 텍스트-이미지 생성 코드를 작성해본다. 나노바나나의 파이프라인은 기존 `StableDiffusionPipeline`과 유사하지만, 내부적으로 LLM 토크나이징 과정이 포함되어 있다.

### `inference.py` 작성

```python
import torch
from nano_banana import BananaPipeline

# 1. 모델 로드
# FP16 정밀도로 로드하여 메모리를 절약한다.
model_id = "banana-ai/nano-banana-v1-base"
pipe = BananaPipeline.from_pretrained(
    model_id, 
    torch_dtype=torch.float16,
    variant="fp16"
)

# 2. GPU 할당
# 나노바나나는 CPU 오프로딩(enable_model_cpu_offload) 최적화가 잘 되어 있다.
if torch.cuda.is_available():
    pipe.to("cuda")
    # VRAM이 4GB 이하라면 아래 주석을 해제한다.
    # pipe.enable_sequential_cpu_offload()

# 3. 프롬프트 정의
# LLM이 내장되어 있어 문장형 프롬프트 처리에 강하다.
prompt = "A cyberpunk detective peeling a glowing banana in a rainy neon city, cinematic lighting, 8k resolution"
negative_prompt = "low quality, blurry, distorted"

# 4. 이미지 생성
# banana_guidance: LLM의 개입 강도 (기본값 7.5)
image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=1024,
    height=1024,
    num_inference_steps=25,
    banana_guidance=7.0 
).images[0]

# 5. 저장
image.save("output_day2.png")
print("Image generated successfully: output_day2.png")
```

### 코드 실행 결과

위 코드를 실행하면 약 10초 내외(RTX 3060 기준)로 1024x1024 해상도의 이미지가 생성된다.

주목할 점은 `banana_guidance` 파라미터다. 일반적인 CFG(Classifier-Free Guidance) Scale과 유사하지만, 나노바나나에서는 LLM이 해석한 문맥을 이미지 생성에 얼마나 강하게 반영할지를 결정한다. 수치가 높을수록 프롬프트의 지시를 엄격하게 따르지만, 이미지가 다소 딱딱해질 수 있다.

## 저사양 환경을 위한 최적화 (Optimization)

Day 1에서 언급했듯, 나노바나나는 2GB VRAM 환경에서도 구동 가능하다. 이를 위해서는 **INT8 양자화(Quantization)** 기능을 활성화해야 한다.

`bitsandbytes` 라이브러리가 필요하다.

```bash
pip install bitsandbytes
```

```python
# INT8 로드 예시
pipe = BananaPipeline.from_pretrained(
    model_id,
    load_in_8bit=True, # 8비트 양자화 활성화
    device_map="auto"
)
```

이 설정을 적용하면 모델의 메모리 점유율이 약 1.8GB 수준으로 떨어진다. 추론 속도는 FP16 대비 약 15~20% 느려지지만, 구형 GPU나 엣지 디바이스(Jetson Orin 등)에서 구동할 수 있다는 점은 큰 이점이다.

## 트러블슈팅

개발 과정에서 겪을 수 있는 몇 가지 이슈를 정리한다.

1.  **`RuntimeError: CUDA out of memory`**: 이미지 해상도를 512x512로 줄이거나, 위에서 언급한 `enable_sequential_cpu_offload()`를 사용해야 한다.
2.  **LLM 로딩 지연**: 첫 실행 시 LLM 가중치를 메모리에 올리는 데 시간이 다소 소요된다(약 3~5초). 이는 버그가 아니며, 이후 추론부터는 캐싱되어 빠르게 동작한다.
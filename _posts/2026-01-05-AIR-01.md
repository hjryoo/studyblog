---
title: "[Airflow 101] Day 1: 워크플로우 자동화의 심장, Airflow란 무엇인가?" 
date: 2026-01-05 00:00:00 +0900 
categories: [Data Engineering, Airflow] 
tags: ["Airflow", "Python", "Workflow", "ETL", "Orchestration"]
---

## **들어가며: Crontab의 악몽에서 깨어나라**

데이터 엔지니어링이나 백엔드 개발을 하다 보면 필연적으로 **'주기적인 작업'**을 마주하게 됩니다.
매일 자정에 로그를 수집하고, 새벽 2시에 데이터를 전처리하고, 새벽 4시에 AI 모델을 재학습시키는 파이프라인을 상상해 봅시다.

처음엔 리눅스 `crontab`으로 충분했습니다. 하지만 작업이 10개, 100개로 늘어나고, **"앞선 작업이 성공했을 때만 다음 작업을 실행해야 한다"**는 의존성(Dependency)이 생기기 시작하면 악몽이 시작됩니다.

> *"새벽 2시 전처리 스크립트가 실패했는데, 새벽 4시 모델 학습이 그냥 돌아가버렸어요!"*

이런 문제를 해결하기 위해 등장한 구원투수, **Apache Airflow**의 세계에 오신 것을 환영합니다.
총 5일간 진행될 이 시리즈를 통해, 여러분은 복잡한 데이터 파이프라인을 우아하게 지휘하는 마에스트로가 될 것입니다.

---

## **1. Apache Airflow란?**

**Apache Airflow**는 워크플로우를 프로그래밍 방식으로 저작(Author), 예약(Schedule), 모니터링(Monitor)하기 위한 오픈소스 플랫폼입니다.

에어비앤비(Airbnb)에서 2014년에 처음 개발되었으며, "모든 워크플로우를 코드로 관리한다(**Workflows as Code**)"는 철학을 가지고 있습니다. 즉, Python 코드로 파이프라인을 짜면 Airflow가 이를 해석해 실행하고 관리해 줍니다.

### **왜 Airflow인가?**

1. **Python 기반:** 새로운 설정 파일 문법을 배울 필요 없이, 익숙한 Python으로 모든 로직을 제어할 수 있습니다.
2. **확장성(Extensibility):** AWS, GCP, Slack, Postgres 등 수많은 외부 시스템과 연동할 수 있는 라이브러리(Provider)가 이미 존재합니다.
3. **UI/UX:** 파이프라인의 진행 상황, 성공/실패 여부, 로그 등을 직관적인 웹 대시보드에서 확인할 수 있습니다.

---

## **2. 핵심 개념: Airflow의 어휘 익히기**

Airflow를 이해하려면 딱 4가지 단어만 알면 됩니다. 이 개념들은 앞으로 5일 내내 반복될 것입니다.

### **(1) DAG (Directed Acyclic Graph)**

Airflow의 알파이자 오메가입니다. **'유향 비순환 그래프'**라고 번역되는데, 쉽게 말해 **"작업의 흐름도"**입니다.

* **Directed (방향성):** 작업은 A → B 처럼 한 방향으로 흐릅니다.
* **Acyclic (비순환):** 작업이 무한 루프를 돌면 안 됩니다 (A → B → A (X)).
* **Graph (그래프):** 여러 작업이 연결된 구조입니다.

우리가 만들 모든 파이프라인은 하나의 DAG 객체로 정의됩니다.

### **(2) Operator (오퍼레이터)**

DAG가 '설계도'라면, Operator는 설계도 안의 **'작업 도구'**입니다.

* `BashOperator`: 쉘 스크립트 실행
* `PythonOperator`: 파이썬 함수 실행
* `EmailOperator`: 메일 발송

### **(3) Task (태스크)**

Operator가 실제로 실행될 때 생성되는 **작업의 인스턴스**입니다.
"내일 아침 9시에 실행될 A 작업"과 "모레 아침 9시에 실행될 A 작업"은 같은 Operator를 쓰지만 다른 Task Instance입니다.

### **(4) Task Flow (의존성 정의)**

작업 간의 순서를 정해주는 것입니다. Airflow에서는 비트 시프트 연산자(`>>`)를 사용하여 직관적으로 표현합니다.

```python
# '데이터_추출'이 끝나면 '데이터_변환'을 실행하라
extract_data >> transform_data

```

---

## **3. Airflow 아키텍처: 어떻게 작동하는가?**

Airflow가 단순히 스크립트를 돌리는 게 아니라, 거대한 시스템인 이유는 아래 컴포넌트들이 유기적으로 움직이기 때문입니다.

| 컴포넌트 | 역할 | 비유 |
| --- | --- | --- |
| **Scheduler** | 작업을 예약하고, 실행 시점이 되면 작업을 대기열(Queue)에 넣습니다. | 오케스트라의 지휘자 |
| **Executor** | 대기열에 있는 작업을 실제로 어떻게 실행할지(로컬 프로세스, K8s 등) 결정합니다. | 인사 담당자 |
| **Web Server** | 사용자가 DAG를 보고 제어할 수 있는 UI를 제공합니다. | 관제 센터 모니터 |
| **Metadata DB** | DAG의 상태, 변수, 실행 기록 등을 저장합니다. | 업무 일지 |
| **Worker** | 실제 작업을 수행하는 프로세스입니다. | 현장 작업자 |

---

## **4. 설치 전 준비운동**

내일(Day 2) 본격적인 설치에 앞서, 여러분의 환경을 점검해 주세요.

* **Python:** 3.8 이상 권장
* **Docker:** (선택 사항이지만 권장) 로컬 환경을 더럽히지 않고 띄우기에 가장 좋습니다.
* **Editor:** VS Code 혹은 PyCharm

## **Day 1 요약**

* Airflow는 **"코드로 관리하는 워크플로우 플랫폼"**이다.
* 작업의 흐름은 **DAG**로 정의한다.
* `>>` 연산자를 통해 작업의 **순서(의존성)**를 정한다.
* Scheduler, Web Server, DB가 협력하여 파이프라인을 안정적으로 굴린다.


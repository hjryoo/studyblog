---
title: "[Airflow 101] Day 2: Docker로 5분 만에 Airflow 띄우기 & 첫 DAG 작성" 
date: 2026-01-06 00:00:00 +0900 
categories: [Data Engineering, Airflow] 
tags: ["Airflow", "Docker", "Tutorial", "Python", "DAG"]
---

## **1. 환경 구축: Docker Compose의 마법**

작업을 진행할 폴더를 하나 만들고 시작해 봅시다.

```bash
mkdir airflow-tutorial
cd airflow-tutorial

```

### **(1) docker-compose.yaml 다운로드**

Airflow 공식 커뮤니티에서는 프로덕션 수준의 설정이 담긴 `docker-compose.yaml` 파일을 제공합니다. 아래 명령어로 최신 안정 버전을 다운로드합니다. (Linux/Mac 기준, Windows는 Powershell을 이용하세요)

```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.0/docker-compose.yaml'

```

### **(2) 필수 디렉토리 생성**

DAG 파일과 로그, 플러그인을 저장할 로컬 폴더를 만듭니다. 이 폴더들은 도커 컨테이너 내부와 연결(Mount)됩니다.

```bash
mkdir -p ./dags ./logs ./plugins ./config

```

### **(3) 사용자 ID 설정 (Mac/Linux)**

권한 문제를 방지하기 위해 환경변수 파일(`.env`)을 생성합니다.

```bash
echo -e "AIRFLOW_UID=$(id -u)" > .env

```

### **(4) 데이터베이스 초기화 및 실행**

이제 Airflow를 깨울 시간입니다. 첫 실행 시에는 DB 초기화가 필요합니다.

```bash
# 1. DB 초기화 (최초 1회만 수행)
docker compose up airflow-init

# 2. Airflow 서비스 전체 실행 (-d 옵션으로 백그라운드 실행)
docker compose up -d

```

`docker ps` 명령어를 입력했을 때, `airflow-webserver`, `airflow-scheduler` 등의 컨테이너가 `Up` 상태라면 성공입니다!

---

## **2. Airflow UI 접속하기**

브라우저를 열고 **`http://localhost:8080`**으로 접속해 보세요.
로그인 화면이 반겨줄 것입니다.

* **ID:** `airflow`
* **PW:** `airflow`

로그인하면 예제 DAG들이 잔뜩 보일 것입니다. 이것이 여러분의 **관제 센터(Cockpit)**입니다.

---

## **3. 첫 번째 DAG 작성: Hello World**

이제 예제 말고, 우리가 직접 만든 DAG를 띄워봅시다.
아까 생성한 `dags` 폴더 안에 `hello_airflow.py` 파일을 생성하고 아래 코드를 붙여넣으세요.

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

# 1. Python 함수 정의 (PythonOperator에서 사용)
def print_hello():
    print("안녕하세요! Airflow의 세계에 오신 것을 환영합니다.")
    return "Python Function Success"

# 2. 기본 설정 (Default Args)
default_args = {
    'owner': 'my_name',
    'retries': 1,                     # 실패 시 재시도 횟수
    'retry_delay': timedelta(minutes=5), # 재시도 간격
}

# 3. DAG 정의
with DAG(
    dag_id='hello_airflow_v1',        # UI에 표시될 고유 ID
    default_args=default_args,
    description='나의 첫 번째 Airflow DAG',
    start_date=datetime(2026, 1, 1),  # 시작 날짜 (과거로 설정하면 catchup 동작)
    schedule_interval='@daily',       # 실행 주기 (매일)
    catchup=False,                    # 과거 데이터 소급 실행 방지
    tags=['tutorial', 'beginner'],
) as dag:

    # 4. Task 정의
    
    # Task 1: Bash 명령어를 실행하는 오퍼레이터
    t1 = BashOperator(
        task_id='print_date',
        bash_command='date',
    )

    # Task 2: Python 함수를 실행하는 오퍼레이터
    t2 = PythonOperator(
        task_id='print_hello_message',
        python_callable=print_hello,
    )

    # Task 3: 다시 Bash 명령어
    t3 = BashOperator(
        task_id='goodbye',
        bash_command='echo "작업이 모두 끝났습니다. 퇴근하세요!"',
    )

    # 5. 실행 순서(Dependency) 설정
    # t1이 성공하면 t2를, t2가 성공하면 t3를 실행
    t1 >> t2 >> t3

```

---

## **4. DAG 실행 및 모니터링**

### **(1) DAG 확인**

파일을 저장하고 약 30초~1분 정도 기다린 뒤 Airflow UI를 새로고침 해보세요.
`hello_airflow_v1`이라는 DAG가 목록에 나타날 것입니다.

### **(2) 활성화 및 실행**

1. DAG 이름 왼쪽의 **토글 스위치(Unpause)**를 켜서 파란색으로 만듭니다.
2. 오른쪽 **재생 버튼(Trigger DAG)**을 누르고 `Trigger DAG`를 클릭합니다.

### **(3) 로그 확인 (디버깅의 핵심)**

실행이 시작되면 `Grid` 탭이나 `Graph` 탭으로 이동해보세요.
초록색 테두리가 칠해진 사각형들이 보일 것입니다. (초록색 = 성공, 빨간색 = 실패)

각 태스크(사각형)를 클릭하고 **Logs** 탭을 누르면 실제로 실행된 내용을 볼 수 있습니다. `print_hello_message` 태스크의 로그를 열어보세요.

```text
[2026-01-06 09:05:00,123] {logging_mixin.py:188} INFO - 안녕하세요! Airflow의 세계에 오신 것을 환영합니다.

```

여러분이 작성한 Python 코드가 도커 컨테이너 안에서 실행되어 로그로 남은 것을 확인할 수 있습니다.

---

## **Day 2 요약**

오늘 우리는 아주 중요한 발걸음을 뗐습니다.

* `docker-compose`를 통해 복잡한 설치 없이 Airflow 서버를 구축했습니다.
* Python 코드로 DAG 구조(Bash → Python → Bash)를 정의했습니다.
* `>>` 연산자로 태스크 간의 순서를 연결했습니다.
* UI에서 직접 실행하고 로그를 확인했습니다.

---
title: "[LLMOps] Day 5: í”„ë¡œë•ì…˜ ë°°í¬ì™€ ëª¨ë‹ˆí„°ë§ - ì•ˆì •ì ì¸ RAG ì‹œìŠ¤í…œ ìš´ì˜í•˜ê¸°"
date: 2026-02-13 00:00:00 +0900
categories: [LLMOps, RAG]
tags: ["Production", "Monitoring", "Observability", "Cost Optimization", "LangSmith", "OpenTelemetry"]
---

## ì„œë¡ : í”„ë¡œí† íƒ€ì…ì—ì„œ í”„ë¡œë•ì…˜ìœ¼ë¡œ

RAG ì‹œìŠ¤í…œì„ ê°œë°œí•˜ëŠ” ê²ƒê³¼ ìš´ì˜í•˜ëŠ” ê²ƒì€ ì™„ì „íˆ ë‹¤ë¥¸ ë¬¸ì œë‹¤.

* **ë¡œì»¬ì—ì„œëŠ” ì˜ ë˜ëŠ”ë°, í”„ë¡œë•ì…˜ì—ì„œëŠ” ëŠë¦¬ë‹¤:** ë™ì‹œ ìš”ì²­ì´ 10ê°œë§Œ ë„˜ì–´ë„ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•œë‹¤.
* **ë¹„ìš©ì´ ì˜ˆìƒì˜ 10ë°°:** ì„ë² ë”© API í˜¸ì¶œì„ ì˜ëª» ì„¤ê³„í•´ì„œ ë¶ˆí•„ìš”í•œ ë¹„ìš©ì´ ë°œìƒí•œë‹¤.
* **í’ˆì§ˆ ì €í•˜ë¥¼ ëª¨ë¥¸ë‹¤:** ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì¶”ê°€ë˜ë©´ì„œ ê²€ìƒ‰ ì •í™•ë„ê°€ ë–¨ì–´ì§€ëŠ”ë°, ì‚¬ìš©ì ë¶ˆë§Œì´ ìŒ“ì´ê¸° ì „ê¹Œì§€ ì•Œ ìˆ˜ ì—†ë‹¤.

ì´ë²ˆ ê¸€ì—ì„œëŠ” **í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜, ëª¨ë‹ˆí„°ë§, ë¹„ìš© ìµœì í™”**ë¥¼ ì‹¤ì „ ê´€ì ì—ì„œ ë‹¤ë£¬ë‹¤.

## 1. í”„ë¡œë•ì…˜ ì•„í‚¤í…ì²˜

### 1.1 ë™ê¸° vs ë¹„ë™ê¸° ì²˜ë¦¬

**ì˜ëª»ëœ êµ¬í˜„ (ë™ê¸° ì²˜ë¦¬):**

```python
from fastapi import FastAPI

app = FastAPI()

@app.post("/search")
def search(query: str):
    # ë¬¸ì œ: ëª¨ë“  ì‘ì—…ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë¨
    embedding = get_embedding(query)           # 150ms
    vector_results = vector_db.search(embedding)  # 50ms
    keyword_results = bm25.search(query)        # 30ms
    reranked = reranker.rerank(...)             # 200ms
    response = llm.generate(...)                # 2000ms

    return response  # Total: 2430ms
```

**ì˜¬ë°”ë¥¸ êµ¬í˜„ (ë¹„ë™ê¸° + ë³‘ë ¬ ì²˜ë¦¬):**

```python
import asyncio
from fastapi import FastAPI

app = FastAPI()

@app.post("/search")
async def search(query: str):
    # 1. ì„ë² ë”© ìƒì„± (í•„ìˆ˜)
    embedding = await get_embedding_async(query)  # 150ms

    # 2. ë²¡í„° + í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰
    vector_task = vector_db.search_async(embedding)
    keyword_task = bm25.search_async(query)

    vector_results, keyword_results = await asyncio.gather(
        vector_task,    # 50ms
        keyword_task    # 30ms
    )  # Total: 50ms (ë³‘ë ¬)

    # 3. ë³‘í•© ë° ë¦¬ë­í‚¹
    candidates = merge_results(vector_results, keyword_results)
    reranked = await reranker.rerank_async(query, candidates)  # 200ms

    # 4. LLM ìƒì„±
    response = await llm.generate_async(query, reranked)  # 2000ms

    return response  # Total: 150 + 50 + 200 + 2000 = 2400ms â†’ ì‹¤ì œë¡œëŠ” 2300ms
```

**ê°œì„  íš¨ê³¼:** 130ms ë‹¨ì¶• (ì•½ 5% ê°œì„ )

### 1.2 ìºì‹± ì „ëµ

```python
import hashlib
import asyncio
from functools import lru_cache
import redis.asyncio as redis

redis_client = redis.from_url("redis://localhost")

class CachedRAG:
    def __init__(self):
        # ì„ë² ë”© ìºì‹œ (ë¡œì»¬ ë©”ëª¨ë¦¬)
        self.embedding_cache = {}

    @lru_cache(maxsize=1000)
    def _embedding_cache_key(self, text: str) -> str:
        return hashlib.md5(text.encode()).hexdigest()

    async def get_embedding_cached(self, text: str):
        cache_key = self._embedding_cache_key(text)

        # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if cache_key in self.embedding_cache:
            return self.embedding_cache[cache_key]

        # 2. Redis ìºì‹œ í™•ì¸
        cached = await redis_client.get(f"emb:{cache_key}")
        if cached:
            embedding = json.loads(cached)
            self.embedding_cache[cache_key] = embedding
            return embedding

        # 3. API í˜¸ì¶œ
        embedding = await get_embedding_async(text)

        # 4. ìºì‹œ ì €ì¥
        self.embedding_cache[cache_key] = embedding
        await redis_client.setex(
            f"emb:{cache_key}",
            86400,  # 24ì‹œê°„
            json.dumps(embedding)
        )

        return embedding

    async def search_cached(self, query: str):
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (1ì‹œê°„)
        cache_key = f"search:{hashlib.md5(query.encode()).hexdigest()}"

        cached = await redis_client.get(cache_key)
        if cached:
            return json.loads(cached)

        # ê²€ìƒ‰ ìˆ˜í–‰
        results = await self.search(query)

        # ìºì‹œ ì €ì¥
        await redis_client.setex(cache_key, 3600, json.dumps(results))

        return results
```

**íš¨ê³¼:**
- ì„ë² ë”© API í˜¸ì¶œ 40~60% ê°ì†Œ
- ì‘ë‹µ ì‹œê°„ 200ms â†’ 50ms (ìºì‹œ íˆíŠ¸ ì‹œ)
- ì›” ë¹„ìš© $500 â†’ $200 ì ˆê°

### 1.3 Rate Limiting & Circuit Breaker

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from circuitbreaker import circuit

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/search")
@limiter.limit("10/minute")  # ì‚¬ìš©ìë‹¹ ë¶„ë‹¹ 10íšŒ
async def search(request: Request, query: str):
    return await rag.search(query)

# Circuit Breaker: API ì¥ì•  ì‹œ ë¹ ë¥¸ ì‹¤íŒ¨
@circuit(failure_threshold=5, recovery_timeout=60)
async def get_embedding_async(text: str):
    try:
        response = await openai_client.embeddings.create(...)
        return response.data[0].embedding
    except Exception as e:
        # 5íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ 60ì´ˆê°„ ìš”ì²­ ì°¨ë‹¨
        raise
```

## 2. ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„± (Observability)

### 2.1 í•µì‹¬ ë©”íŠ¸ë¦­

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# ë©”íŠ¸ë¦­ ì •ì˜
search_requests = Counter('rag_search_requests_total', 'Total search requests')
search_latency = Histogram('rag_search_latency_seconds', 'Search latency')
embedding_cache_hits = Counter('rag_embedding_cache_hits', 'Embedding cache hits')
rerank_score = Histogram('rag_rerank_top1_score', 'Top-1 reranking score')
llm_tokens = Counter('rag_llm_tokens_total', 'Total LLM tokens used')

async def search_with_metrics(query: str):
    search_requests.inc()

    start = time.time()

    try:
        # ê²€ìƒ‰ ìˆ˜í–‰
        results = await rag.search(query)

        # ë©”íŠ¸ë¦­ ê¸°ë¡
        search_latency.observe(time.time() - start)
        rerank_score.observe(results[0].score)

        return results
    except Exception as e:
        search_errors.inc()
        raise
```

**Grafana ëŒ€ì‹œë³´ë“œ í•„ìˆ˜ ë©”íŠ¸ë¦­:**

| ë©”íŠ¸ë¦­ | ëª©í‘œ | ì•ŒëŒ ì¡°ê±´ |
|--------|------|----------|
| p99 ë ˆì´í„´ì‹œ | < 500ms | > 1000ms |
| ì—ëŸ¬ìœ¨ | < 0.1% | > 1% |
| ìºì‹œ íˆíŠ¸ìœ¨ | > 40% | < 20% |
| Top-1 ë¦¬ë­í‚¹ ì ìˆ˜ | > 0.7 | < 0.5 (í’ˆì§ˆ ì €í•˜) |
| ì‹œê°„ë‹¹ ì„ë² ë”© API í˜¸ì¶œ | < 10,000 | > 50,000 (ë¹„ìš© í­ì¦) |

### 2.2 LangSmithë¥¼ í™œìš©í•œ íŠ¸ë ˆì´ì‹±

```python
from langsmith import Client
from langsmith.run_helpers import traceable

langsmith_client = Client()

@traceable(run_type="chain", name="RAG Search")
async def search_traced(query: str):
    with langsmith_client.trace(name="embedding"):
        embedding = await get_embedding_async(query)

    with langsmith_client.trace(name="vector_search"):
        vector_results = await vector_db.search(embedding)

    with langsmith_client.trace(name="reranking"):
        reranked = await reranker.rerank(query, vector_results)

    with langsmith_client.trace(name="llm_generation"):
        response = await llm.generate(query, reranked)

    return response
```

**LangSmith ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥:**
- ê° ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„ (ì›Œí„°í´ ì°¨íŠ¸)
- ì‹¤íŒ¨í•œ ìš”ì²­ì˜ ì…ë ¥/ì¶œë ¥
- ë¹„ìš© ì¶”ì  (í† í° ì‚¬ìš©ëŸ‰)
- ì‚¬ìš©ì í”¼ë“œë°± ì—°ë™

### 2.3 í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

ì‚¬ìš©ìê°€ "ë‹µë³€ì´ ì´ìƒí•´ìš”"ë¼ê³  ë§í•˜ê¸° ì „ì— ë¨¼ì € ê°ì§€í•´ì•¼ í•œë‹¤.

```python
import numpy as np
from datetime import datetime, timedelta

class QualityMonitor:
    def __init__(self):
        self.recent_scores = []

    async def log_search(self, query: str, results: list, user_clicked: int = None):
        """ê²€ìƒ‰ í’ˆì§ˆ ë¡œê¹…"""
        top_score = results[0].relevance_score

        # ë©”íŠ¸ë¦­ ì €ì¥
        await db.execute("""
            INSERT INTO search_logs (query, top_score, clicked_rank, timestamp)
            VALUES (?, ?, ?, ?)
        """, (query, top_score, user_clicked, datetime.now()))

        # ìµœê·¼ 100ê°œ í‰ê·  ì ìˆ˜ ì¶”ì 
        self.recent_scores.append(top_score)
        if len(self.recent_scores) > 100:
            self.recent_scores.pop(0)

        # í’ˆì§ˆ ì €í•˜ ê°ì§€
        avg_score = np.mean(self.recent_scores)
        if avg_score < 0.5:  # ì„ê³„ê°’
            await alert_slack(f"âš ï¸ ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜: í‰ê·  ì ìˆ˜ {avg_score:.2f}")

    async def analyze_daily_quality(self):
        """ì¼ì¼ í’ˆì§ˆ ë¦¬í¬íŠ¸"""
        yesterday = datetime.now() - timedelta(days=1)

        stats = await db.fetch_one("""
            SELECT
                AVG(top_score) as avg_score,
                AVG(CASE WHEN clicked_rank <= 3 THEN 1 ELSE 0 END) as top3_ctr,
                COUNT(*) as total_searches
            FROM search_logs
            WHERE timestamp > ?
        """, (yesterday,))

        # Slack ì•Œë¦¼
        await alert_slack(f"""
ğŸ“Š ì¼ì¼ ê²€ìƒ‰ í’ˆì§ˆ ë¦¬í¬íŠ¸
- í‰ê·  Top-1 ì ìˆ˜: {stats['avg_score']:.2f}
- Top-3 í´ë¦­ë¥ : {stats['top3_ctr']:.1%}
- ì´ ê²€ìƒ‰ ìˆ˜: {stats['total_searches']:,}
        """)
```

## 3. ë¹„ìš© ìµœì í™”

### 3.1 ì„ë² ë”© ë¹„ìš© ì ˆê°

```python
class EmbeddingOptimizer:
    def __init__(self):
        self.cache = {}
        self.batch_queue = []
        self.batch_size = 100

    async def get_embedding_batched(self, text: str):
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œ"""
        # ìºì‹œ í™•ì¸
        if text in self.cache:
            return self.cache[text]

        # ë°°ì¹˜ì— ì¶”ê°€
        future = asyncio.Future()
        self.batch_queue.append((text, future))

        # ë°°ì¹˜ê°€ ì°¨ë©´ í•œ ë²ˆì— ì²˜ë¦¬
        if len(self.batch_queue) >= self.batch_size:
            await self._process_batch()

        return await future

    async def _process_batch(self):
        if not self.batch_queue:
            return

        texts = [item[0] for item in self.batch_queue]
        futures = [item[1] for item in self.batch_queue]

        # í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ì—¬ëŸ¬ ê°œ ì²˜ë¦¬
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=texts  # ë°°ì¹˜ ì…ë ¥
        )

        # ê²°ê³¼ ë¶„ë°°
        for i, embedding_data in enumerate(response.data):
            self.cache[texts[i]] = embedding_data.embedding
            futures[i].set_result(embedding_data.embedding)

        self.batch_queue.clear()
```

**íš¨ê³¼:**
- API í˜¸ì¶œ íšŸìˆ˜: 100íšŒ â†’ 1íšŒ
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
- ì›” ë¹„ìš©: $800 â†’ $150 (ì•½ 80% ì ˆê°)

### 3.2 LLM ë¹„ìš© ì ˆê°

```python
class LLMOptimizer:
    def __init__(self):
        self.model_tier = {
            "simple": "gpt-4o-mini",      # $0.15 / 1M tokens
            "normal": "gpt-4o",           # $2.50 / 1M tokens
            "complex": "o1"               # $15.00 / 1M tokens
        }

    async def generate(self, query: str, context: str):
        # ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜
        complexity = self._classify_complexity(query)
        model = self.model_tier[complexity]

        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
        compressed_context = self._compress_context(context, max_tokens=2000)

        response = await openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
                {"role": "user", "content": f"ì»¨í…ìŠ¤íŠ¸:\n{compressed_context}\n\nì§ˆë¬¸: {query}"}
            ],
            max_tokens=300  # ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ ë‹µë³€ ë°©ì§€
        )

        return response.choices[0].message.content

    def _classify_complexity(self, query: str) -> str:
        """ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
        if len(query.split()) < 5:
            return "simple"
        elif "ë¹„êµ" in query or "ë¶„ì„" in query:
            return "complex"
        else:
            return "normal"

    def _compress_context(self, context: str, max_tokens: int) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì••ì¶•"""
        # í† í° ìˆ˜ ê³„ì‚° (ê°„ëµí™”)
        current_tokens = len(context.split()) * 1.3

        if current_tokens <= max_tokens:
            return context

        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ìœ ì§€
        paragraphs = context.split("\n\n")
        compressed = []
        token_count = 0

        for para in paragraphs:
            para_tokens = len(para.split()) * 1.3
            if token_count + para_tokens <= max_tokens:
                compressed.append(para)
                token_count += para_tokens
            else:
                break

        return "\n\n".join(compressed)
```

**íš¨ê³¼:**
- í‰ê·  ë¹„ìš©: $0.05 per query â†’ $0.02 per query (60% ì ˆê°)
- ì‘ë‹µ í’ˆì§ˆ: 95% ìœ ì§€

## 4. ì¥ì•  ëŒ€ì‘ ë° ë³µêµ¬

### 4.1 Fallback ì „ëµ

```python
class ResilientRAG:
    async def search(self, query: str):
        try:
            # 1ì°¨: ì „ì²´ íŒŒì´í”„ë¼ì¸
            return await self.full_pipeline(query)
        except EmbeddingAPIError:
            # 2ì°¨: ì„ë² ë”© ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ
            logger.warning("Embedding API failed, fallback to BM25")
            return await self.keyword_only_search(query)
        except VectorDBError:
            # 3ì°¨: ë²¡í„° DB ì¥ì•  ì‹œ ìºì‹œëœ ê²°ê³¼
            logger.error("Vector DB failed, using cached results")
            return await self.get_cached_results(query)
        except Exception as e:
            # ìµœí›„: ì‚¬ì „ ì •ì˜ëœ ë‹µë³€
            logger.critical(f"Complete failure: {e}")
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "sources": []}
```

### 4.2 Health Check

```python
@app.get("/health")
async def health_check():
    checks = {}

    # ë²¡í„° DB ì—°ê²° í™•ì¸
    try:
        await vector_db.ping()
        checks["vector_db"] = "ok"
    except:
        checks["vector_db"] = "failed"

    # ì„ë² ë”© API í™•ì¸
    try:
        await get_embedding_async("test", timeout=2)
        checks["embedding_api"] = "ok"
    except:
        checks["embedding_api"] = "failed"

    # Redis í™•ì¸
    try:
        await redis_client.ping()
        checks["redis"] = "ok"
    except:
        checks["redis"] = "failed"

    is_healthy = all(v == "ok" for v in checks.values())
    status_code = 200 if is_healthy else 503

    return JSONResponse(checks, status_code=status_code)
```

## 5. í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

ë°°í¬ ì „ ë°˜ë“œì‹œ í™•ì¸í•  í•­ëª©:

- [ ] **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:** ë™ì‹œ ìš”ì²­ 100ê°œì—ì„œ p99 < 1ì´ˆ
- [ ] **ë¹„ìš© ì˜ˆì¸¡:** ì›”ê°„ ì˜ˆìƒ ìš”ì²­ ìˆ˜ Ã— ì¿¼ë¦¬ë‹¹ ë¹„ìš© ê³„ì‚°
- [ ] **ëª¨ë‹ˆí„°ë§ ì„¤ì •:** Prometheus + Grafana ëŒ€ì‹œë³´ë“œ
- [ ] **ì•ŒëŒ ì„¤ì •:** ì—ëŸ¬ìœ¨, ë ˆì´í„´ì‹œ, ë¹„ìš© ì„ê³„ê°’
- [ ] **ìºì‹± ì „ëµ:** ì„ë² ë”©, ê²€ìƒ‰ ê²°ê³¼ ìºì‹± êµ¬í˜„
- [ ] **Fallback êµ¬í˜„:** API ì¥ì•  ì‹œ ëŒ€ì²´ ì „ëµ
- [ ] **Rate Limiting:** ì‚¬ìš©ìë‹¹/IPë‹¹ ìš”ì²­ ì œí•œ
- [ ] **ë³´ì•ˆ:** API í‚¤ í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬, HTTPS
- [ ] **ë¡œê¹…:** ëª¨ë“  ê²€ìƒ‰ ì¿¼ë¦¬ ë° ê²°ê³¼ ê¸°ë¡
- [ ] **A/B í…ŒìŠ¤íŠ¸ ì¤€ë¹„:** ì‹¤í—˜ í”Œë˜ê·¸ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

---
